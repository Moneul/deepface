{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "from deepface.basemodels import VGGFace, OpenFace, Facenet, FbDeepFace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import pymsgbox\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepface를 통한 본인인증"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 얼굴 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './data/Mini_Project/Face'  # 폴더 경로 설정\n",
    "file_list = glob.glob(folder_path + '/*')  # 폴더 내의 파일 목록 얻기\n",
    "file_count = len(file_list)  # 파일 개수 계산\n",
    "print(f\"폴더 내 파일 개수: {file_count}\")\n",
    "\n",
    "webcam = cv2.VideoCapture(0)\n",
    "if not webcam.isOpened():\n",
    "    print('웹캠을 찾을 수 없습니다.')\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    status, frame = webcam.read()\n",
    "    if status:\n",
    "        cv2.imshow('test', frame)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    \n",
    "    # c 키 누르면 사진 저장\n",
    "    if key == ord('c'):\n",
    "        username = file_count+1\n",
    "        img_captured = cv2.imwrite(f'./data/Mini_Project/Face/face_00{username}.jpg', frame)\n",
    "        pymsgbox.alert('등록완료', '알림')\n",
    "        break\n",
    "    \n",
    "    # esc 혹은 창닫기 버튼 누르면 종료\n",
    "    if key == 27 or cv2.getWindowProperty('test', cv2.WND_PROP_VISIBLE) < 1:\n",
    "        break\n",
    "\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c키 눌러서 인증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGGFace.loadModel()\n",
    "\n",
    "count = 0\n",
    "value = 0.25\n",
    "distance_list = []\n",
    "input_size = model.layers[0].input_shape[1:3]\n",
    "\n",
    "backends = ['opencv', 'ssd', 'dlib', 'mtcnn']\n",
    "\n",
    "webcam = cv2.VideoCapture(0)\n",
    "if not webcam.isOpened():\n",
    "    print('웹캠을 찾을 수 없습니다.')\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    status, frame = webcam.read()\n",
    "    if status:\n",
    "        cv2.imshow('test', frame)\n",
    "    \n",
    "    Face_Images = os.path.join(os.getcwd(), './data/Mini_Project/Face') # 등록된 얼굴 이미지 폴더 지정\n",
    "    Face_Images_List = []\n",
    "    for root, dir, files in os.walk(Face_Images): # 파일 목록 가져오기\n",
    "        for file in files:\n",
    "            if file.endswith('jpeg') or file.endswith('jpg') or file.endswith('png'): # 이미지 파일 필터링\n",
    "                image_path = os.path.join(Face_Images, file)\n",
    "                # print(image_path)\n",
    "                Face_Images_List.append(cv2.imread(image_path))\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    \n",
    "    # c 키 누르면 사진 저장\n",
    "    if key == ord('c'):\n",
    "        \n",
    "        # 프레임을 변수에 저장 및 예측\n",
    "        # img_captured = frame\n",
    "        img2 = DeepFace.detectFace(frame, detector_backend=backends[3])\n",
    "        img2 = np.expand_dims(img2, axis=0)\n",
    "        img2_representation = model.predict(img2)[0,:]\n",
    "        \n",
    "        # 등록되어있던 이미지들 불러오기\n",
    "        for idx in range(len(Face_Images_List)):\n",
    "    \n",
    "            img1 = DeepFace.detectFace(Face_Images_List[idx], detector_backend=backends[3])\n",
    "            img1 = np.expand_dims(img1, axis=0)\n",
    "            img1_representation = model.predict(img1)[0,:]\n",
    "\n",
    "            distance_vector = np.square(img1_representation - img2_representation)\n",
    "            distance = np.sqrt(distance_vector.sum())\n",
    "            distance_list.append(distance)\n",
    "            # print('Euclidean distance : ', distance)\n",
    "\n",
    "        if any(item <= value for item in distance_list):\n",
    "            pymsgbox.alert('인증 되셨습니다', '알림')\n",
    "        else:\n",
    "            pymsgbox.alert(f'다시 시도해주세요. {4-count}회 남음', '알림')\n",
    "            count += 1\n",
    "            if count == 5:\n",
    "                pymsgbox.alert(f'{count}회 시도했습니다. 30초후 다시 시도해주세요', '알림')\n",
    "                count = 0\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "    \n",
    "    # esc 혹은 창닫기 버튼 누르면 종료\n",
    "    if key == 27 or cv2.getWindowProperty('test', cv2.WND_PROP_VISIBLE) < 1:\n",
    "        break\n",
    "\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c키 없이 자동 인증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGGFace.loadModel()\n",
    "\n",
    "count = 0\n",
    "value = 0.25\n",
    "distance_list = []\n",
    "input_size = model.layers[0].input_shape[1:3]\n",
    "\n",
    "backends = ['opencv', 'ssd', 'dlib', 'mtcnn']\n",
    "\n",
    "webcam = cv2.VideoCapture(0)\n",
    "if not webcam.isOpened():\n",
    "    print('웹캠을 찾을 수 없습니다.')\n",
    "    exit()\n",
    "\n",
    "capture_interval = 7  # 캡처 간격(초)\n",
    "last_capture_time = time.time() - capture_interval  # 이전 캡처 시간\n",
    "\n",
    "while True:\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - last_capture_time\n",
    "\n",
    "    status, frame = webcam.read()\n",
    "    if status:\n",
    "        cv2.imshow('test', frame)\n",
    "        \n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    \n",
    "    # 지정된 시간 간격마다 자동으로 캡처\n",
    "    \n",
    "    if elapsed_time >= capture_interval:\n",
    "        img2 = DeepFace.detectFace(frame, detector_backend=backends[3])\n",
    "        img2 = np.expand_dims(img2, axis=0)\n",
    "        img2_representation = model.predict(img2)[0, :]\n",
    "\n",
    "        Face_Images = os.path.join(os.getcwd(), './data/Mini_Project/Face')  # 등록된 얼굴 이미지 폴더 지정\n",
    "        Face_Images_List = []\n",
    "        for root, dir, files in os.walk(Face_Images):  # 파일 목록 가져오기\n",
    "            for file in files:\n",
    "                if file.endswith(('jpeg', 'jpg', 'png')):  # 이미지 파일 필터링\n",
    "                    image_path = os.path.join(Face_Images, file)\n",
    "                    Face_Images_List.append(cv2.imread(image_path))\n",
    "\n",
    "        for idx in range(len(Face_Images_List)):\n",
    "            img1 = DeepFace.detectFace(Face_Images_List[idx], detector_backend=backends[3])\n",
    "            img1 = np.expand_dims(img1, axis=0)\n",
    "            img1_representation = model.predict(img1)[0, :]\n",
    "\n",
    "            distance_vector = np.square(img1_representation - img2_representation)\n",
    "            distance = np.sqrt(distance_vector.sum())\n",
    "            distance_list.append(distance)\n",
    "\n",
    "        if any(item <= value for item in distance_list):\n",
    "            pymsgbox.alert('인증 되셨습니다', '알림')\n",
    "            \n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "        else:\n",
    "            pymsgbox.alert(f'다시 시도해주세요. {4 - count}회 남음', '알림')\n",
    "            count += 1\n",
    "            if count == 5:\n",
    "                pymsgbox.alert(f'{count}회 시도했습니다. 30초 후 다시 시도해주세요', '알림')\n",
    "                count = 0\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "\n",
    "        last_capture_time = current_time  # 캡처 시간 업데이트\n",
    "\n",
    "    # c 키 누르면 사진 저장\n",
    "    if key == ord('c'):\n",
    "        img2 = DeepFace.detectFace(frame, detector_backend=backends[3])\n",
    "        img2 = np.expand_dims(img2, axis=0)\n",
    "        img2_representation = model.predict(img2)[0, :]\n",
    "        break\n",
    "\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# har 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# 얼굴 인식용 xml 파일\n",
    "face_classifier = cv2.CascadeClassifier('./data/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# 전체 사진에서 얼굴 부위만 잘라내어 리턴\n",
    "def face_extractor(img):\n",
    "    # 흑백처리\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # 얼굴 찾기\n",
    "    faces = face_classifier.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "    # 찾은 얼굴이 없으면 None으로 리턴\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    # 모든 얼굴 잘라내기\n",
    "    cropped_faces = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "        cropped_faces.append(cropped_face)\n",
    "    # cropped_faces 리턴\n",
    "    return cropped_faces\n",
    "\n",
    "# 카메라 실행\n",
    "cap = cv2.VideoCapture(0)\n",
    "# 저장할 이미지 카운트 변수\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    # 카메라로부터 사진 1장 얻기\n",
    "    ret, frame = cap.read()\n",
    "    # 얼굴 감지하여 얼굴들만 가져오기\n",
    "    faces = face_extractor(frame)\n",
    "    \n",
    "    if faces is not None:\n",
    "        # 얼굴 개수만큼 반복\n",
    "        for face in faces:\n",
    "            count += 1\n",
    "            # 얼굴 이미지 크기를 200x200으로 조정\n",
    "            face_resized = cv2.resize(face, (200, 200))\n",
    "            # 조정된 이미지를 흑백으로 변환\n",
    "            face_gray = cv2.cvtColor(face_resized, cv2.COLOR_BGR2GRAY)\n",
    "            # faces 폴더에 jpg 파일로 저장\n",
    "            file_name_path = './data/Mini_Project/naver/' + str(count) + '.jpg'\n",
    "            cv2.imwrite(file_name_path, face_gray)\n",
    "            # 화면에 얼굴과 현재 저장 개수 표시\n",
    "            cv2.putText(face_resized, str(count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.imshow('Face Cropper', face_resized)\n",
    "    \n",
    "    else:\n",
    "        print(\"Face not Found\")\n",
    "    \n",
    "    if cv2.waitKey(1) == 13 or count == 100:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print('Collecting Samples Complete!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "data_path = './data/Mini_Project/naver/'\n",
    "#faces폴더에 있는 파일 리스트 얻기 \n",
    "onlyfiles = [f for f in listdir(data_path) if isfile(join(data_path,f))]\n",
    "#데이터와 매칭될 라벨 변수 \n",
    "Training_Data, Labels = [], []\n",
    "#파일 개수 만큼 루프 \n",
    "for i, files in enumerate(onlyfiles):    \n",
    "    image_path = data_path + onlyfiles[i]\n",
    "    #이미지 불러오기 \n",
    "    images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    #이미지 파일이 아니거나 못 읽어 왔다면 무시\n",
    "    if images is None:\n",
    "        continue    \n",
    "    #Training_Data 리스트에 이미지를 바이트 배열로 추가 \n",
    "    Training_Data.append(np.asarray(images, dtype=np.uint8))\n",
    "    #Labels 리스트엔 카운트 번호 추가 \n",
    "    Labels.append(i)\n",
    "\n",
    "#훈련할 데이터가 없다면 종료.\n",
    "if len(Labels) == 0:\n",
    "    print(\"There is no data to train.\")\n",
    "    exit()\n",
    "\n",
    "#Labels를 32비트 정수로 변환\n",
    "Labels = np.asarray(Labels, dtype=np.int32)\n",
    "#모델 생성 \n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "#학습 시작 \n",
    "model.train(np.asarray(Training_Data), np.asarray(Labels))\n",
    "print(\"Model Training Complete!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "data_path = './data/Mini_Project/naver/'\n",
    "onlyfiles = [f for f in listdir(data_path) if isfile(join(data_path,f))]\n",
    "Training_Data, Labels = [], []\n",
    "for i, files in enumerate(onlyfiles):\n",
    "    image_path = data_path + onlyfiles[i]\n",
    "    images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if images is None:\n",
    "        continue    \n",
    "    Training_Data.append(np.asarray(images, dtype=np.uint8))\n",
    "    Labels.append(i)\n",
    "if len(Labels) == 0:\n",
    "    print(\"There is no data to train.\")\n",
    "    exit()\n",
    "Labels = np.asarray(Labels, dtype=np.int32)\n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "model.train(np.asarray(Training_Data), np.asarray(Labels))\n",
    "print(\"Model Training Complete!!!!!\")\n",
    "\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./data/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img, size=0.5):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=3)\n",
    "    if len(faces) == 0:\n",
    "        return img, None\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 255), 2)\n",
    "        roi = img[y:y + h, x:x + w]\n",
    "        roi = cv2.resize(roi, (200, 200))\n",
    "    return img, roi\n",
    "\n",
    "\n",
    "\n",
    "#카메라 열기 \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    #카메라로 부터 사진 한장 읽기 \n",
    "    ret, frame = cap.read()\n",
    "    # 얼굴 검출 시도 \n",
    "    image, face = face_detector(frame)\n",
    "    try:\n",
    "        #검출된 사진을 흑백으로 변환 \n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "        #위에서 학습한 모델로 예측시도\n",
    "        result = model.predict(face)\n",
    "        #result[1]은 신뢰도이고 0에 가까울수록 자신과 같다는 뜻이다. \n",
    "        if result[1] < 500:\n",
    "            #????? 어쨋든 0~100표시하려고 한듯 \n",
    "            confidence = int(100*(1-(result[1])/300))\n",
    "            # 유사도 화면에 표시 \n",
    "            display_string = str(confidence)+'% 사용자와 일치'\n",
    "        cv2.putText(image,display_string,(100,120), cv2.FONT_HERSHEY_COMPLEX,1,(250,120,255),2)\n",
    "        #75 보다 크면 동일 인물로 간주해 UnLocked! \n",
    "        if confidence > 50:\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX # 폰트 지정\n",
    "            cv2.putText(image, \"잠금 해제\", (250, 450), font, 1, (0, 255, 0), 2)\n",
    "            cv2.imshow('Face Cropper', image)\n",
    "        else:\n",
    "           #75 이하면 타인.. Locked!!! \n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX # 폰트 지정\n",
    "            cv2.putText(image, \"잠금\", (250, 450), font, 1, (0, 0, 255), 2)\n",
    "            cv2.imshow('Face Cropper', image)\n",
    "    except:\n",
    "        #얼굴 검출 안됨\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX # 폰트 지정\n",
    "        cv2.putText(image, \"Face Not Found\", (250, 450), font, 1, (255, 0, 0), 2)\n",
    "        cv2.imshow('Face Cropper', image)\n",
    "        pass\n",
    "    if cv2.waitKey(1)==13:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yolo v8을 통한 segmantation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 1 book, 6.5ms\n",
      "Speed: 1.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 6.4ms\n",
      "Speed: 1.0ms preprocess, 6.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.4ms\n",
      "Speed: 1.0ms preprocess, 7.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 5.3ms\n",
      "Speed: 1.1ms preprocess, 5.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 5.8ms\n",
      "Speed: 2.6ms preprocess, 5.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.0ms\n",
      "Speed: 1.1ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 5.1ms\n",
      "Speed: 1.1ms preprocess, 5.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 6.4ms\n",
      "Speed: 0.0ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 6.8ms\n",
      "Speed: 1.0ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 4.1ms\n",
      "Speed: 2.1ms preprocess, 4.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 book, 5.2ms\n",
      "Speed: 1.1ms preprocess, 5.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 book, 6.6ms\n",
      "Speed: 1.3ms preprocess, 6.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 book, 6.2ms\n",
      "Speed: 1.4ms preprocess, 6.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 4.5ms\n",
      "Speed: 1.1ms preprocess, 4.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 5.6ms\n",
      "Speed: 1.1ms preprocess, 5.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n-seg.pt')\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame)\n",
    "        # for result in results:\n",
    "        #     masks = results[0].masks  # Masks object\n",
    "        #     masks.xy  # x, y segments (pixels), List[segment] * N\n",
    "        #     masks.xyn  # x, y segments (normalized), List[segment] * N\n",
    "        #     masks.data  # raw masks tensor, (N, H, W) or masks.masks \n",
    "        #     print(masks.xy)\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot(boxes=False)\n",
    "        # masks = result[0].masks\n",
    "        # print(masks.xy)\n",
    "        # print(masks.xyn)\n",
    "        # print(masks.data)\n",
    "        # print(result)\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Found https:\\ultralytics.com\\images\\bus.jpg locally at bus.jpg\n",
      "image 1/1 c:\\AISW\\video\\bus.jpg: 640x480 4 persons, 1 bus, 1 skateboard, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model = YOLO('yolov8n-seg.pt')\n",
    "results = model('https://ultralytics.com/images/bus.jpg', imgsz=640)\n",
    "img = cv2.imread('bus.jpg')\n",
    "img = cv2.resize(img, (480, 640))\n",
    "\n",
    "for result in results:\n",
    "    for mask in result.masks:\n",
    "        m = torch.squeeze(mask.data)\n",
    "        composite = torch.stack((m, m, m), 2)\n",
    "        tmp = img * composite.cpu().numpy().astype(np.uint8)\n",
    "        cv2.imshow(\"result\", tmp)\n",
    "        cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Found https:\\ultralytics.com\\images\\bus.jpg locally at bus.jpg\n",
      "image 1/1 c:\\AISW\\video\\bus.jpg: 640x480 4 persons, 1 bus, 1 skateboard, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model = YOLO('yolov8n-seg.pt')\n",
    "results = model('https://ultralytics.com/images/bus.jpg', imgsz=640)\n",
    "img = cv2.imread('bus.jpg')\n",
    "img = cv2.resize(img, (480, 640))\n",
    "\n",
    "# Create a black background image\n",
    "background = np.zeros(img.shape, dtype=np.uint8)\n",
    "\n",
    "for result in results:\n",
    "    for mask in result.masks:\n",
    "        m = torch.squeeze(mask.data)\n",
    "        composite = torch.stack((m, m, m), 2)\n",
    "        tmp = img * composite.cpu().numpy().astype(np.uint8)\n",
    "        background += tmp\n",
    "\n",
    "# Display the masked image\n",
    "cv2.imshow(\"Masked Image\", background)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 1 chair, 8.1ms\n",
      "Speed: 1.1ms preprocess, 8.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 7.2ms\n",
      "Speed: 1.1ms preprocess, 7.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.3ms\n",
      "Speed: 1.4ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 11.8ms\n",
      "Speed: 1.1ms preprocess, 11.8ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 6.5ms\n",
      "Speed: 1.0ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 9.7ms\n",
      "Speed: 2.1ms preprocess, 9.7ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 7.7ms\n",
      "Speed: 1.1ms preprocess, 7.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.2ms\n",
      "Speed: 2.1ms preprocess, 7.2ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 7.7ms\n",
      "Speed: 1.1ms preprocess, 7.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 8.2ms\n",
      "Speed: 1.2ms preprocess, 8.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 5.9ms\n",
      "Speed: 1.1ms preprocess, 5.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 6.4ms\n",
      "Speed: 1.3ms preprocess, 6.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 7.8ms\n",
      "Speed: 2.0ms preprocess, 7.8ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 5.6ms\n",
      "Speed: 1.2ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 7.6ms\n",
      "Speed: 1.0ms preprocess, 7.6ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 5.7ms\n",
      "Speed: 2.3ms preprocess, 5.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 5.4ms\n",
      "Speed: 1.0ms preprocess, 5.4ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 5.9ms\n",
      "Speed: 1.0ms preprocess, 5.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 5.3ms\n",
      "Speed: 1.1ms preprocess, 5.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 5.8ms\n",
      "Speed: 1.1ms preprocess, 5.8ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 5.5ms\n",
      "Speed: 2.2ms preprocess, 5.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 6.4ms\n",
      "Speed: 1.3ms preprocess, 6.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 5.5ms\n",
      "Speed: 1.1ms preprocess, 5.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 5.6ms\n",
      "Speed: 1.2ms preprocess, 5.6ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 5.7ms\n",
      "Speed: 1.1ms preprocess, 5.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 5.6ms\n",
      "Speed: 1.0ms preprocess, 5.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 6.2ms\n",
      "Speed: 2.1ms preprocess, 6.2ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 5.4ms\n",
      "Speed: 2.0ms preprocess, 5.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 7.2ms\n",
      "Speed: 2.1ms preprocess, 7.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 6.5ms\n",
      "Speed: 1.1ms preprocess, 6.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 6.1ms\n",
      "Speed: 1.0ms preprocess, 6.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 6.1ms\n",
      "Speed: 0.0ms preprocess, 6.1ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 5.7ms\n",
      "Speed: 1.0ms preprocess, 5.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.0ms\n",
      "Speed: 0.9ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 6.4ms\n",
      "Speed: 1.0ms preprocess, 6.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 12.0ms\n",
      "Speed: 4.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 6.0ms\n",
      "Speed: 2.1ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 12.8ms\n",
      "Speed: 1.0ms preprocess, 12.8ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# YOLOv8 모델 로드\n",
    "model = YOLO('yolov8n-seg.pt')\n",
    "\n",
    "# 웹캠을 위한 비디오 캡처 객체 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# 적용할 배경\n",
    "bgimg = cv2.imread('./yolobg.jpg')\n",
    "\n",
    "# 비디오 프레임을 반복하여 처리\n",
    "while cap.isOpened():\n",
    "    # 비디오에서 프레임 읽기\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # YOLOv8를 사용하여 프레임에 대한 추론 실행\n",
    "        results = model(frame)\n",
    "\n",
    "        # 검정색 배경 이미지 생성\n",
    "        background = np.zeros(frame.shape, dtype=np.uint8)\n",
    "\n",
    "        for result in results:\n",
    "            for mask in result.masks:\n",
    "                # 마스크 데이터에서 차원 축소\n",
    "                m = torch.squeeze(mask.data)\n",
    "                # 마스크를 RGB 형식으로 변환하여 컴포지트(composite) 텐서 생성\n",
    "                composite = torch.stack((m, m, m), 2)\n",
    "                # 프레임과 컴포지트를 곱하여 마스크 영역 추출\n",
    "                tmp = frame * composite.cpu().numpy().astype(np.uint8)\n",
    "                # 추출된 마스크 영역을 배경에 누적\n",
    "                background += tmp\n",
    "                \n",
    "\n",
    "        # 마스킹된 이미지 출력\n",
    "        cv2.imshow(\"마스킹된 이미지\", background)\n",
    "\n",
    "        # 'q' 키가 눌리면 루프 종료\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # 비디오의 끝에 도달하면 루프 종료\n",
    "        break\n",
    "\n",
    "# 비디오 캡처 객체 해제 및 화면 창 닫기\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yolov8 세그먼트를 이용한 화상회의 배경 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 2 chairs, 99.1ms\n",
      "Speed: 2.0ms preprocess, 99.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 9.0ms\n",
      "Speed: 1.2ms preprocess, 9.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 7.1ms\n",
      "Speed: 1.2ms preprocess, 7.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 7.7ms\n",
      "Speed: 2.4ms preprocess, 7.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 5.2ms\n",
      "Speed: 2.5ms preprocess, 5.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 5.5ms\n",
      "Speed: 1.0ms preprocess, 5.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 8.1ms\n",
      "Speed: 1.0ms preprocess, 8.1ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 5.5ms\n",
      "Speed: 2.0ms preprocess, 5.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 5.1ms\n",
      "Speed: 1.0ms preprocess, 5.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 6.5ms\n",
      "Speed: 1.1ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# YOLOv8 모델 로드\n",
    "model = YOLO('yolov8n-seg.pt')\n",
    "\n",
    "# 웹캠을 위한 비디오 캡처 객체 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# 적용할 배경\n",
    "bgimg = cv2.imread('./yolobg.jpg')\n",
    "\n",
    "# 비디오 프레임을 반복하여 처리\n",
    "while cap.isOpened():\n",
    "    # 비디오에서 프레임 읽기\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # YOLOv8를 사용하여 프레임에 대한 추론 실행\n",
    "        results = model(frame)\n",
    "\n",
    "        # 검정색 배경 이미지 생성\n",
    "        background = np.zeros(frame.shape, dtype=np.uint8)\n",
    "\n",
    "        for result in results:\n",
    "            for mask in result.masks:\n",
    "                # 마스크 데이터에서 차원 축소\n",
    "                m = torch.squeeze(mask.data)\n",
    "                # 마스크를 RGB 형식으로 변환하여 컴포지트(composite) 텐서 생성\n",
    "                composite = torch.stack((m, m, m), 2)\n",
    "                # 프레임과 컴포지트를 곱하여 마스크 영역 추출\n",
    "                tmp = frame * composite.cpu().numpy().astype(np.uint8)\n",
    "                # 추출된 마스크 영역을 배경에 누적\n",
    "                background += tmp\n",
    "\n",
    "        # 배경 이미지를 원하는 배경으로 마스킹\n",
    "        bgimg_resized = cv2.resize(bgimg, (frame.shape[1], frame.shape[0]))\n",
    "        background = np.where(background == 0, bgimg_resized, background)\n",
    "\n",
    "        # 마스킹된 이미지 출력\n",
    "        cv2.imshow(\"마스킹된 이미지\", background)\n",
    "\n",
    "        # 'q' 키가 눌리면 루프 종료\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # 비디오의 끝에 도달하면 루프 종료\n",
    "        break\n",
    "\n",
    "# 비디오 캡처 객체 해제 및 화면 창 닫기\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "ip_address = \"rtsp://admin:admin123@192.168.1.2:554/cam/realmonitor?channel=1&subtype=0&unicast=true&proto=Onvif\"\n",
    "cap = cv2.VideoCapture(ip_address)\n",
    "model = YOLO('yolov8n-seg.pt')\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc('m','p','4','v')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "show_boxes = False\n",
    "\n",
    "out = cv2.VideoWriter('output.mp4', fourcc, fps,(frame_width,frame_height),True )\n",
    "while(cap.isOpened()):\n",
    "    ret,frame = cap.read()\n",
    "    if ret == True:\n",
    "        person_found = False\n",
    "        results = model(frame, imgsz=640, stream=True, verbose=False)\n",
    "        for result in results:\n",
    "            for box in result.boxes.cpu().numpy():\n",
    "                if show_boxes:\n",
    "                    r = box.xyxy[0].astype(int)\n",
    "                    cv2.rectangle(frame, r[:2], r[2:], (255, 255, 255), 2)\n",
    "                cls = int(box.cls[0])\n",
    "                if cls == 0:\n",
    "                    person_found = True\n",
    "\n",
    "        if person_found:\n",
    "            out.write(frame)\n",
    "        \n",
    "        cv2.imshow('Frame', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
